{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78658e47-fd6b-4231-bbb2-e12b1e41bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef461dff-d70f-4da8-8262-837cc212efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8aa6036-a5c7-4c68-b065-9a090a209349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99836407-761b-48a8-af75-bb96f048b823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "num_classes = 5\n",
    "IMAGE_SHAPE = [224, 224]  # Input shape for MobileNetV2\n",
    "batch_size = 32\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15fc5680-7cff-46f3-8520-86270bf92741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 model with pre-trained ImageNet weights\n",
    "mobilenet = MobileNetV2(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52777cee-3106-45d6-bf09-6c06876b9638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers\n",
    "for layer in mobilenet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59c297b8-7bd9-453a-9676-d2e6996ce52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the MobileNetV2 base\n",
    "x = Flatten()(mobilenet.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bbfff9d-b8f8-4010-babf-a4a2e1f41c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model(inputs=mobilenet.input, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d854e2-da7d-49ad-a7e0-5cecb07cb577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "trdata = ImageDataGenerator()\n",
    "train_data_gen = trdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "tsdata = ImageDataGenerator()\n",
    "test_data_gen = tsdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e53ad2-90c0-4fdd-80ec-bfed9e90ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_steps_per_epoch = np.ceil(train_data_gen.samples / batch_size)\n",
    "validation_steps_per_epoch = np.ceil(test_data_gen.samples / batch_size)\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(train_data_gen.samples / batch_size)),\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=int(np.ceil(test_data_gen.samples / batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "print('Training Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd7971a-dac4-49d0-a6c6-07430693ca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "Y_pred = model.predict(test_data_gen, test_data_gen.samples // batch_size)\n",
    "val_preds = np.argmax(Y_pred, axis=1)\n",
    "val_trues = test_data_gen.classes\n",
    "print(metrics.classification_report(val_trues, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54697c-7b37-4669-8e42-09a16d1816f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(val_trues, val_preds)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942900d-af7e-408b-bff0-88d81adb03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "keras_file = \"replace with file path\"\n",
    "tf.keras.models.save_model(model, keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52614cb4-9260-4d01-9bc8-ec4d2ca5dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95544e45-cb93-4a5c-bb16-7ea8539220ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "num_classes = 5\n",
    "IMAGE_SHAPE = [224, 224]  # Input shape for ResNet50\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2187396-1e98-4d33-afbe-3c6310118f80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load ResNet50 model with pre-trained ImageNet weights\n",
    "resnet = ResNet50(input_shape=(224, 224, 3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46d5d59d-c8ea-40e8-9d2e-86df7ca91879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2f553cc-c014-4384-badb-868e7049cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the ResNet50 base\n",
    "x = Flatten()(resnet.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03aaebb4-1e54-4953-bf1b-79b263d1e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7d4aa2-778a-4e5d-8306-194ce2908129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "trdata = ImageDataGenerator()\n",
    "train_data_gen = trdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "tsdata = ImageDataGenerator()\n",
    "test_data_gen = tsdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4b4ed-ddfc-4682-a01f-905ac7741016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_steps_per_epoch = np.ceil(train_data_gen.samples / batch_size)\n",
    "validation_steps_per_epoch = np.ceil(test_data_gen.samples / batch_size)\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(train_data_gen.samples / batch_size)),\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=int(np.ceil(test_data_gen.samples / batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "print('Training Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303c513-0c5b-4e9a-962a-d5664cd84021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "Y_pred = model.predict(test_data_gen, test_data_gen.samples // batch_size)\n",
    "val_preds = np.argmax(Y_pred, axis=1)\n",
    "val_trues = test_data_gen.classes\n",
    "print(metrics.classification_report(val_trues, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31253ae7-3b9a-480c-abd4-43817f59e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(val_trues, val_preds)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07abfe5c-2b2f-4cec-8d0e-7467748b9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b4806-cf4e-40aa-b14c-3be5b2ab20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "keras_file = \"replace with file path\"\n",
    "tf.keras.models.save_model(model, keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a3bfc8bc-29b2-4626-827a-bc49de664e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76d6c8b8-7d1c-4f3e-80c2-54173513e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "num_classes = 5\n",
    "IMAGE_SHAPE = [224, 224]  # Input shape for ResNet101\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a694106-77f6-424f-838d-3ebb8e7c3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet101 model with pre-trained ImageNet weights\n",
    "resnet = ResNet101(input_shape=(224, 224, 3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59120236-71e9-45c8-8cbf-56658092d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "662533c6-d373-42ba-a4b6-daee23cee673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the ResNet101 base\n",
    "x = Flatten()(resnet.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96f9e958-ef14-44c7-b5c5-2612eada84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b67e8-1cbb-4c9a-83f9-09e9a6f341c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "trdata = ImageDataGenerator()\n",
    "train_data_gen = trdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "tsdata = ImageDataGenerator()\n",
    "test_data_gen = tsdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116f545b-af7f-441f-ab4d-95f12101fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_steps_per_epoch = np.ceil(train_data_gen.samples / batch_size)\n",
    "validation_steps_per_epoch = np.ceil(test_data_gen.samples / batch_size)\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(train_data_gen.samples / batch_size)),\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=int(np.ceil(test_data_gen.samples / batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "print('Training Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c4f22-4874-4650-ac17-5a3aabb1fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "Y_pred = model.predict(test_data_gen, test_data_gen.samples // batch_size)\n",
    "val_preds = np.argmax(Y_pred, axis=1)\n",
    "val_trues = test_data_gen.classes\n",
    "print(metrics.classification_report(val_trues, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666dae13-20c7-4ca2-8e4e-8759cef5897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(val_trues, val_preds)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e99634-6332-4c56-9911-a8cac9b127a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "keras_file = \"replace with file path\"\n",
    "tf.keras.models.save_model(model, keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa64de24-e39f-4831-a235-3652a6f27f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d24454d-18a3-495e-98cc-c5b599552538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "num_classes = 5\n",
    "IMAGE_SHAPE = [224, 224]  # Input shape for DenseNet121\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f6aa2-ecd7-4721-9b09-1cfd380740d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DenseNet121 model with pre-trained ImageNet weights\n",
    "densenet = DenseNet121(input_shape=(224, 224, 3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdfdf2b7-f741-40c4-b23b-cdb3f60b3397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "949b3b88-9996-4964-acdf-3e6099e4aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the DenseNet121 base\n",
    "x = Flatten()(densenet.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ebae5cc-1817-44fa-b982-b1a21b4b0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model(inputs=densenet.input, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56c079-549a-430d-9994-af8937861fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "trdata = ImageDataGenerator()\n",
    "train_data_gen = trdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "tsdata = ImageDataGenerator()\n",
    "test_data_gen = tsdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d77eb8-a22a-4ce2-ab92-a0b113a636c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_steps_per_epoch = np.ceil(train_data_gen.samples / batch_size)\n",
    "validation_steps_per_epoch = np.ceil(test_data_gen.samples / batch_size)\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(train_data_gen.samples / batch_size)),\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=int(np.ceil(test_data_gen.samples / batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "print('Training Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac16b3-ff20-4ad7-b698-9ba429fb91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "Y_pred = model.predict(test_data_gen, test_data_gen.samples // batch_size)\n",
    "val_preds = np.argmax(Y_pred, axis=1)\n",
    "val_trues = test_data_gen.classes\n",
    "print(metrics.classification_report(val_trues, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001121f-9f76-4338-8b56-66ed95e7c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(val_trues, val_preds)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b7edd6-d97f-490a-9790-40b297382175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "keras_file = \"replace with file path\"\n",
    "tf.keras.models.save_model(model, keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50666990-2c8a-4bdf-9d3e-39a684150ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dab9f8b0-6183-495e-b72b-8b6e7ab86668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "num_classes = 5\n",
    "IMAGE_SHAPE = [224, 224]  # Input shape for InceptionV3\n",
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa20ed6-8245-45f9-823c-63eec68680ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load InceptionV3 model with pre-trained ImageNet weights\n",
    "inception = InceptionV3(input_shape=(224, 224, 3), weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c7f543-a7eb-40a4-88ff-b8c8a507f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers\n",
    "for layer in inception.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4ab9b1-747d-4a86-b4ab-ad0385181a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers on top of the inceptionv3 base\n",
    "x = Flatten()(inception.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(num_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a8f90f-5cfc-495e-a98c-a12435ca6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Model(inputs=inception.input, outputs=x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a37ab-e727-4548-a4f1-a8311baf6a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "trdata = ImageDataGenerator()\n",
    "train_data_gen = trdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "tsdata = ImageDataGenerator()\n",
    "test_data_gen = tsdata.flow_from_directory(\n",
    "    directory=\"replace with file path\",\n",
    "    target_size=IMAGE_SHAPE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae55387-adad-49fc-a0d8-29536f450230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "training_steps_per_epoch = np.ceil(train_data_gen.samples / batch_size)\n",
    "validation_steps_per_epoch = np.ceil(test_data_gen.samples / batch_size)\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch=int(np.ceil(train_data_gen.samples / batch_size)),\n",
    "    validation_data=test_data_gen,\n",
    "    validation_steps=int(np.ceil(test_data_gen.samples / batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1\n",
    ")\n",
    "print('Training Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a8d98-6a64-472f-b9d7-01ee6c7db3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and evaluation\n",
    "Y_pred = model.predict(test_data_gen, test_data_gen.samples // batch_size)\n",
    "val_preds = np.argmax(Y_pred, axis=1)\n",
    "val_trues = test_data_gen.classes\n",
    "print(metrics.classification_report(val_trues, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b8b94-8fcf-4bf7-a4a3-82d0cfcbadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(val_trues, val_preds)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sn.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5246b2d-26cd-470b-bf20-32c2f2038a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "keras_file = \"replace with file path\"\n",
    "tf.keras.models.save_model(model, keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c168ece-682d-4eb6-8fcc-d4106c144de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c32f77-0cb0-431f-9819-d344d7accb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bbd77-e0ee-4f06-95ea-0609225f1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11e3c83-6ffb-47e3-9d06-fba3ce424fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffb158-46b7-4b3f-a468-6f1c45c19187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fb48eb-0e9e-4c5c-b8f8-0ec0325bf1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3cde49-d5f3-41f2-9363-2ebc2ce9de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "loaded_model = load_model(\"replace with file path\")\n",
    "print(\"âœ… Model Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa581450-a633-45ec-a197-d32eb958bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img):\n",
    "    img = img.resize(IMAGE_SHAPE)  # Resize image\n",
    "    img = np.array(img) / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)  # Expand dimensions for batch processing\n",
    "    \n",
    "    prediction = loaded_model.predict(img)\n",
    "    print(prediction)\n",
    "    predicted_class = class_labels[np.argmax(prediction)]  # Get class label\n",
    "    \n",
    "    return f\"Predicted Class: {predicted_class}\"\n",
    "\n",
    "IMAGE_SHAPE = (224, 224)\n",
    "\n",
    "#class_labels = [1700,1492,1937,1290,1364]\n",
    "class_labels = [1364,1492,1937,1290,1700]\n",
    "\n",
    "# Create Gradio Interface\n",
    "gradio_interface = gr.Interface(\n",
    "    fn=predict_image, \n",
    "    inputs=gr.Image(type=\"pil\"), \n",
    "    outputs=\"text\",\n",
    "    title=\"GUI for Author Recognition\",\n",
    "    description=\"Upload an image and the model will predict which scholar it belongs to.\"\n",
    ")\n",
    "\n",
    "# Launch Gradio App\n",
    "gradio_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c86269-a756-4607-9fcd-ef7483a41cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d3ce1-508d-4faa-a64c-92e08196adb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6779d-7366-48a4-8a79-0c24134dd640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c030f6-5506-426e-b366-09ba64bafd28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f00c78-2481-4aee-9938-37413dccfef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f6458-47a6-4fa4-937d-bee728d19b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4e349-143d-45a6-b858-c0d1da04c652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ab2b40-69f9-49af-9a53-256bd5233eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e11c5b-6318-4d5a-a454-76bccb4d9f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c3c7a-8b38-4513-91be-04eb4ac16346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a642a8-8a39-46f2-89f5-005c12a705f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa4738-0878-47bf-87d3-2f0c49b2d638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32107c64-4962-4fc9-994a-2070fe4af017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b192e9-5607-473a-9d25-88efc0c3ff23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d068e-6272-49f8-8920-38a879dffae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
